{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/homohl/speech-emotion-recognition-in-tinyml?scriptVersionId=123748190\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<h1 style=\"text-align: center;\">Speech Emotion Recognition in TinyML</h1>","metadata":{}},{"cell_type":"markdown","source":"## Introduction\nIn this notebook, we explore the development of a speech emotion recognition (SER) model using deep neural networks and its conversion into a TinyML model. We chose the Long Short-Term Memory (LSTM) architecture and Mel-frequency cepstral coefficients (MFCCs) as they are adept at learning from sequences and have been proven to be well-suited for SER tasks [[1]]. Furthermore, we leverage TinyML to enable machine learning models to operate on resource-constrained devices like microcontrollers, offering advantages for various applications, including mobile devices, IoT devices, and wearables.\n\nIn this application, we consider the circumplex model in affective computing, which is a two-dimensional model representing emotions in terms of valence (pleasantness) and arousal (intensity) [[2]]. Based on this model and the implementation scenarios, we drop the categories \"Fear\" and \"Disgusted\" and merge \"Angry\" and \"Sad\" emotions into the same category, \"Unpleasant,\" due to their similar attributes of valence and arousal. Consequently, we focus on classifying four emotions: Happy, Surprised, Neutral, and Unpleasant.\n\nThe dataset employed in this notebook is a combination of three datasets: RAVDESS [[3]], TESS [[4]], and SAVEE [[5]]. We will load and preprocess the data before exploring and visualizing wave plots and spectrograms for different emotions. We utilize data augmentation techniques such as noise addition, stretching, and pitching to expand the dataset and enhance the model's performance.\n\nUpon completing the standard training and validation pipeline, we convert the model into a TensorFlow Lite (TFLite) format, quantize it to reduce the size, and compare its inference time with that of the original model.\n\nReference:<br>\n[[1]] Kumbhar, Harshawardhan S., and Sheetal U. Bhandari. \"Speech emotion recognition using MFCC features and LSTM network.\" 2019 5th International Conference On Computing, Communication, Control And Automation (ICCUBEA). IEEE, 2019.<br>\n[[2]] Russell, James A. \"A circumplex model of affect.\" Journal of personality and social psychology 39.6 (1980): 1161.<br>\n[[3]] Livingstone, Steven R., and Frank A. Russo. \"The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English.\" PloS one 13.5 (2018): e0196391.<br>\n[[4]] Dupuis, Kate, and M. Kathleen Pichora-Fuller. \"Toronto emotional speech set (tess)-younger talker_happy.\" (2010).<br>\n[[5]] Jackson, Philip, and SJUoSG Haq. \"Surrey audio-visual expressed emotion (savee) database.\" University of Surrey: Guildford, UK (2014).\n\n[1]: https://doi.org/10.1109/ICCUBEA47591.2019.9129067\n[2]: https://doi.org/10.1037/h0077714\n[3]: https://doi.org/10.1371/journal.pone.0196391\n[4]: https://doi.org/10.5683/SP2/E8H2MF\n[5]: https://openresearch.surrey.ac.uk/esploro/outputs/journalArticle/Surrey-audio-visual-expressed-emotion-savee-database/99635364402346","metadata":{}},{"cell_type":"markdown","source":"## Importing Libraries\nInclude the necessary libraries for data manipulation, visualization, and model building:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport os\nimport sys\n\nimport librosa\nimport librosa.display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\nfrom IPython.display import Audio\n\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Flatten, Dropout\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:12:00.099963Z","iopub.execute_input":"2023-03-28T19:12:00.100423Z","iopub.status.idle":"2023-03-28T19:12:08.278968Z","shell.execute_reply.started":"2023-03-28T19:12:00.100362Z","shell.execute_reply":"2023-03-28T19:12:08.27776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset\nWe will use three different datasets for speech emotion recognition: RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song), TESS (Toronto Emotional Speech Set), and SAVEE (Surrey Audio-Visual Expressed Emotion). These datasets offer several benefits that contribute to the development of more robust and accurate SER models:\n\n1. Variety of emotional expressions:<br> These datasets encompass a wide range of emotions, such as happy, sad, angry, fearful, surprised, disgusted, and neutral. This variety helps train models to recognize and distinguish subtle differences between various emotional expressions, enhancing their performance.\n\n2. Multiple speakers:<br> Including multiple speakers with different accents, genders, and speaking styles provide a more diverse and representative speech data sample. This diversity helps models generalize to real-world scenarios, making them more effective in handling speech data from various sources.\n\n3. High-quality recordings:<br> The audio files in these datasets are recorded with high-quality equipment, resulting in clear and consistent audio samples, allowing models to focus on the emotional content of the speech without being hindered by noise or other artifacts.","metadata":{}},{"cell_type":"code","source":"# Paths for data.\nRavdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\nTess = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\nSavee = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/\"","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:12:08.281017Z","iopub.execute_input":"2023-03-28T19:12:08.281476Z","iopub.status.idle":"2023-03-28T19:12:08.286459Z","shell.execute_reply.started":"2023-03-28T19:12:08.281429Z","shell.execute_reply":"2023-03-28T19:12:08.28552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ravdess_directory_list = os.listdir(Ravdess)\n\nfile_emotion = []\nfile_path = []\nfor dir in ravdess_directory_list:\n    # as their are 20 different actors in our previous directory we need to extract files for each actor.\n    actor = os.listdir(Ravdess + dir)\n    for file in actor:\n        part = file.split('.')[0]\n        part = part.split('-')\n        # third part in each file represents the emotion associated to that file.\n        file_emotion.append(int(part[2]))\n        file_path.append(Ravdess + dir + '/' + file)\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nRavdess_df = pd.concat([emotion_df, path_df], axis=1)\n\n# Change integers to actual emotions.\nRavdess_df.Emotions.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n\nRavdess_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:12:08.288996Z","iopub.execute_input":"2023-03-28T19:12:08.289538Z","iopub.status.idle":"2023-03-28T19:12:08.681298Z","shell.execute_reply.started":"2023-03-28T19:12:08.289489Z","shell.execute_reply":"2023-03-28T19:12:08.680546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tess_directory_list = os.listdir(Tess)\n\nfile_emotion = []\nfile_path = []\n\nfor dir in tess_directory_list:\n    directories = os.listdir(Tess + dir)\n    for file in directories:\n        part = file.split('.')[0]\n        part = part.split('_')[2]\n        if part=='ps':\n            file_emotion.append('surprise')\n        else:\n            file_emotion.append(part)\n        file_path.append(Tess + dir + '/' + file)\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nTess_df = pd.concat([emotion_df, path_df], axis=1)\n\nTess_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:12:08.682903Z","iopub.execute_input":"2023-03-28T19:12:08.683444Z","iopub.status.idle":"2023-03-28T19:12:09.386664Z","shell.execute_reply.started":"2023-03-28T19:12:08.683394Z","shell.execute_reply":"2023-03-28T19:12:09.385654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"savee_directory_list = os.listdir(Savee)\n\nfile_emotion = []\nfile_path = []\n\nfor file in savee_directory_list:\n    file_path.append(Savee + file)\n    part = file.split('_')[1]\n    ele = part[:-6]\n    if ele=='a':\n        file_emotion.append('angry')\n    elif ele=='d':\n        file_emotion.append('disgust')\n    elif ele=='f':\n        file_emotion.append('fear')\n    elif ele=='h':\n        file_emotion.append('happy')\n    elif ele=='n':\n        file_emotion.append('neutral')\n    elif ele=='sa':\n        file_emotion.append('sad')\n    else:\n        file_emotion.append('surprise')\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nSavee_df = pd.concat([emotion_df, path_df], axis=1)\n\nSavee_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:12:09.388166Z","iopub.execute_input":"2023-03-28T19:12:09.388493Z","iopub.status.idle":"2023-03-28T19:12:09.517469Z","shell.execute_reply.started":"2023-03-28T19:12:09.388461Z","shell.execute_reply":"2023-03-28T19:12:09.516504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aggregated_data = pd.concat([Ravdess_df, Tess_df, Savee_df], axis = 0)\n\n# Shuffle the dataframe using the sample method\naggregated_data = aggregated_data.sample(frac=1).reset_index(drop=True) \n\n# Drop rows where Emotions is 'fear' or 'disgust'\naggregated_data = aggregated_data[~aggregated_data['Emotions'].isin(['fear', 'disgust'])]\n\n# Drop rows where Emotions is \"sad\" and \"angry\" and replace them with \"unpleasant\"\naggregated_data = aggregated_data.drop(aggregated_data[aggregated_data['Emotions'] == 'sad'].sample(frac=0.4).index)\naggregated_data = aggregated_data.drop(aggregated_data[aggregated_data['Emotions'] == 'angry'].sample(frac=0.4).index)\naggregated_data['Emotions'] = aggregated_data['Emotions'].replace(['sad', 'angry'], 'unpleasant')\n\naggregated_data.to_csv(\"data_path.csv\",index=False)\naggregated_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:12:09.518732Z","iopub.execute_input":"2023-03-28T19:12:09.519053Z","iopub.status.idle":"2023-03-28T19:12:09.561167Z","shell.execute_reply.started":"2023-03-28T19:12:09.519015Z","shell.execute_reply":"2023-03-28T19:12:09.56014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('Count of Emotions', size=16)\nsns.countplot(aggregated_data.Emotions)\nplt.ylabel('Count', size=12)\nplt.xlabel('Emotions', size=12)\nsns.despine(top=True, right=True, left=False, bottom=False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:12:09.562592Z","iopub.execute_input":"2023-03-28T19:12:09.562911Z","iopub.status.idle":"2023-03-28T19:12:09.750562Z","shell.execute_reply.started":"2023-03-28T19:12:09.562878Z","shell.execute_reply":"2023-03-28T19:12:09.749515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Exploration\nIn this section, you've plotted waveforms and spectrograms for emotions data, as well as applied different audio transformations like noise addition, time stretching, and pitch shifting to provide an overview of the audio data properties.","metadata":{}},{"cell_type":"code","source":"def create_waveplot(data, sr, e):\n    plt.figure(figsize=(10, 3))\n    plt.title('Waveplot for {} emotion'.format(e), size=15)\n    librosa.display.waveplot(data, sr=sr)\n    plt.show()\n\ndef create_spectrogram(data, sr, e):\n    X = librosa.stft(data)\n    Xdb = librosa.amplitude_to_db(abs(X))\n    plt.figure(figsize=(12, 3))\n    plt.title('Spectrogram for {} emotion'.format(e), size=15)\n    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')   \n    plt.colorbar()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:12:09.752991Z","iopub.execute_input":"2023-03-28T19:12:09.753287Z","iopub.status.idle":"2023-03-28T19:12:09.760851Z","shell.execute_reply.started":"2023-03-28T19:12:09.753257Z","shell.execute_reply":"2023-03-28T19:12:09.759721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def noise(data):\n    noise_amp = 0.5*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n\ndef stretch(data, rate=0.8):\n    return librosa.effects.time_stretch(data, rate)\n\ndef pitch(data, sampling_rate, pitch_factor=0.7):\n    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:12:09.762897Z","iopub.execute_input":"2023-03-28T19:12:09.763216Z","iopub.status.idle":"2023-03-28T19:12:09.771804Z","shell.execute_reply.started":"2023-03-28T19:12:09.763185Z","shell.execute_reply":"2023-03-28T19:12:09.771021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion='happy'\npath = np.array(aggregated_data.Path[aggregated_data.Emotions==emotion])[1]\ndata, sample_rate = librosa.load(path)\ndata = librosa.resample(data, sample_rate, 16000)\nnoised_data = noise(data)\n\ncreate_waveplot(data, sample_rate, emotion)\ncreate_spectrogram(data, sample_rate, emotion)\nAudio(data=noised_data, rate=16000)\n\n# Key Speech Element Span\n# Ravdess_df: 0.8-2.8s -> 2.0s\n#    Tess_df: 0.3-1.8s -> 1.5s\n#   Savee_df: 0.6-3.3s -> 2.7s","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:12:09.772974Z","iopub.execute_input":"2023-03-28T19:12:09.773387Z","iopub.status.idle":"2023-03-28T19:12:11.140905Z","shell.execute_reply.started":"2023-03-28T19:12:09.773343Z","shell.execute_reply":"2023-03-28T19:12:11.140041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 2, figsize=(20,8))\nplt.subplots_adjust(hspace=0.4)\n\naxs[0, 0].set_title('Original Signal', size=20)\nlibrosa.display.waveplot(y=data, sr=sample_rate, ax=axs[0, 0])\n\naxs[0, 1].set_title('Noised Signal', size=20)\nnoise_data = noise(data)\nlibrosa.display.waveplot(y=noise_data, sr=sample_rate, ax=axs[0, 1])\n\naxs[1, 0].set_title('Streched Signal', size=20)\nstretch_data = stretch(data)\nlibrosa.display.waveplot(y=stretch_data, sr=sample_rate, ax=axs[1, 0])\n\naxs[1, 1].set_title('Pitched Signal', size=20)\npitch_data = pitch(data, sample_rate)\nlibrosa.display.waveplot(y=pitch_data, sr=sample_rate, ax=axs[1, 1])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:12:11.14204Z","iopub.execute_input":"2023-03-28T19:12:11.142554Z","iopub.status.idle":"2023-03-28T19:12:13.073901Z","shell.execute_reply.started":"2023-03-28T19:12:11.142508Z","shell.execute_reply":"2023-03-28T19:12:13.07286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Pre-processing\nThis section preprocesses audio data for speech emotion recognition. It encoded emotion labels, extracts features using MFCC, and augments data with audio transformations. The data is stored in a CSV file and split into training, validation, and testing sets for model training.","metadata":{}},{"cell_type":"code","source":"labels = {'neutral':0, 'happy':1, 'surprise':2, 'unpleasant': 3}\naggregated_data.replace({'Emotions':labels},inplace=True)\naggregated_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:12:13.075605Z","iopub.execute_input":"2023-03-28T19:12:13.076196Z","iopub.status.idle":"2023-03-28T19:12:13.091314Z","shell.execute_reply.started":"2023-03-28T19:12:13.076148Z","shell.execute_reply":"2023-03-28T19:12:13.090445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_MFCC = 13\nN_FFT = 2048\nHOP_LENGTH = 512\nSAMPLE_RATE = 22050\nDOWN_SAMPLE_RATE = 16000\nSAMPLE_NUM = aggregated_data.shape[0]\n\ndata = {\n        \"labels\": [],\n        \"features\": []\n    }\n\ndef extract_features(data, sample_rate):\n    mfcc = librosa.feature.mfcc(data, sample_rate, n_mfcc=NUM_MFCC, n_fft=N_FFT, hop_length=HOP_LENGTH)\n    feature = mfcc.T\n    return feature\n\nfor i in range(SAMPLE_NUM):\n    for j in range(2):\n        data['labels'].append(aggregated_data.iloc[i,0])\n    signal, sample_rate = librosa.load(aggregated_data.iloc[i,1], sr=SAMPLE_RATE)\n    \n    # Cropping & Resampling\n    start_time = 0.4  # Start time in seconds\n    end_time = 1.9  # End time in seconds\n    start_frame = int(start_time * sample_rate)\n    end_frame = int(end_time * sample_rate)\n    signal = signal[start_frame:end_frame]\n    signal = librosa.resample(signal, sample_rate, DOWN_SAMPLE_RATE)\n    \n    # Add noise\n    signal = noise(signal)\n    res1 = extract_features(signal, DOWN_SAMPLE_RATE)\n    data[\"features\"].append(np.array(res1))\n    \n    # Stretch and shift pitch\n    new_data = stretch(signal)[:24000]\n    data_stretch_pitch = pitch(new_data, DOWN_SAMPLE_RATE)\n    res2 = extract_features(data_stretch_pitch, DOWN_SAMPLE_RATE)\n    data[\"features\"].append(np.array(res2))\n    \n    if i % 100 == 0:\n        print(f'Processing Data: {i}/{SAMPLE_NUM}')","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:12:13.092472Z","iopub.execute_input":"2023-03-28T19:12:13.092887Z","iopub.status.idle":"2023-03-28T19:29:05.549236Z","shell.execute_reply.started":"2023-03-28T19:12:13.092856Z","shell.execute_reply":"2023-03-28T19:29:05.547645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Features = pd.DataFrame()\nFeatures['features'] = data[\"features\"]\nFeatures['labels'] = data[\"labels\"]\nFeatures.to_csv('Features.csv', index=False)\nFeatures.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:29:05.551853Z","iopub.execute_input":"2023-03-28T19:29:05.552787Z","iopub.status.idle":"2023-03-28T19:29:38.226234Z","shell.execute_reply.started":"2023-03-28T19:29:05.552721Z","shell.execute_reply":"2023-03-28T19:29:38.225133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.asarray(Features['features'])\ny = np.asarray(Features[\"labels\"])\n\n# Pad Features to make them of equal length\nX = tf.keras.preprocessing.sequence.pad_sequences(X)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:29:38.22767Z","iopub.execute_input":"2023-03-28T19:29:38.22799Z","iopub.status.idle":"2023-03-28T19:29:38.266204Z","shell.execute_reply.started":"2023-03-28T19:29:38.22795Z","shell.execute_reply":"2023-03-28T19:29:38.265273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\nX_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2)\n\nprint(f'Training Data:{X_train.shape} with label {y_train.shape}')\nprint(f'Validate Data:{X_validation.shape} with label {y_validation.shape}')\nprint(f' Testing Data:{X_test.shape} with label {y_test.shape}')","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:29:38.267577Z","iopub.execute_input":"2023-03-28T19:29:38.267956Z","iopub.status.idle":"2023-03-28T19:29:38.289342Z","shell.execute_reply.started":"2023-03-28T19:29:38.267913Z","shell.execute_reply":"2023-03-28T19:29:38.288188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training\nThis sequential model consists of LSTM layers that capture long-term dependencies in the audio data and the dense layers that transform extracted features to classification.","metadata":{}},{"cell_type":"code","source":"def build_model(input_shape):\n    model = tf.keras.Sequential()\n\n    model.add(LSTM(128, input_shape=input_shape, return_sequences=True))\n    model.add(LSTM(64))\n    \n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.3))\n\n    model.add(Dense(4, activation='softmax'))\n\n    return model\n\n# Create network\ninput_shape = (47,13)\nmodel = build_model(input_shape)\n\n# Compile model\noptimiser = tf.keras.optimizers.Adam(learning_rate=0.001)\n\nmodel.compile(optimizer=optimiser,\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:29:38.290719Z","iopub.execute_input":"2023-03-28T19:29:38.291215Z","iopub.status.idle":"2023-03-28T19:29:38.920208Z","shell.execute_reply.started":"2023-03-28T19:29:38.291169Z","shell.execute_reply":"2023-03-28T19:29:38.919321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run the training process\nEPOCHS = 20\nhistory = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=32, epochs=EPOCHS)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-03-28T19:29:38.921343Z","iopub.execute_input":"2023-03-28T19:29:38.921706Z","iopub.status.idle":"2023-03-28T19:33:22.6409Z","shell.execute_reply.started":"2023-03-28T19:29:38.921674Z","shell.execute_reply":"2023-03-28T19:33:22.640062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Create a new directory called 'my_data' to store the model\noutput_dir = '/kaggle/working/Models'\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nmodel.save('Models/Speech-Emotion-Recognition-Model.h5')\nprint('Save the Tensorflow model!')","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:33:22.643601Z","iopub.execute_input":"2023-03-28T19:33:22.643907Z","iopub.status.idle":"2023-03-28T19:33:22.683152Z","shell.execute_reply.started":"2023-03-28T19:33:22.643879Z","shell.execute_reply":"2023-03-28T19:33:22.681897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation\nWe evaluate the speech emotion recognition model by measuring test accuracy, plotting loss and accuracy graphs, printing a classification report, and creating a confusion matrix. These tools help assess the model's performance, identify areas for improvement, and enhance its accuracy and generalization capabilities.","metadata":{}},{"cell_type":"code","source":"test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Test Accuracy: \", test_acc*100 , \"%\")\n\n\nepochs = [i for i in range(EPOCHS)]\nfig, ax = plt.subplots(1, 2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\n\nfig.set_size_inches(20, 6)\nax[0].plot(epochs, train_loss, label='Training Loss')\nax[0].plot(epochs, val_loss, label='Validating Loss')\nax[0].set_title('Training & Validating Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs, train_acc, label='Training Accuracy')\nax[1].plot(epochs, val_acc, label='Validating Accuracy')\nax[1].set_title('Training & Validating Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:33:22.685095Z","iopub.execute_input":"2023-03-28T19:33:22.685551Z","iopub.status.idle":"2023-03-28T19:33:23.646861Z","shell.execute_reply.started":"2023-03-28T19:33:22.685501Z","shell.execute_reply":"2023-03-28T19:33:23.645801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test)\ny_pred = np.argmax(y_pred, axis=1)\nlabel_names = list(labels.keys())\nprint(classification_report(y_test, y_pred, target_names=label_names))","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:33:23.648214Z","iopub.execute_input":"2023-03-28T19:33:23.648524Z","iopub.status.idle":"2023-03-28T19:33:24.942848Z","shell.execute_reply.started":"2023-03-28T19:33:23.648492Z","shell.execute_reply":"2023-03-28T19:33:24.941701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model's performance on speech emotion recognition, as shown by the confusion matrix, achieved an overall accuracy of 0.74. For individual emotions, the f1-scores were: Neutral (0.82), Happy (0.71), Surprise (0.74), and Unpleasant (0.67). These scores indicate that the model performs best in recognizing neutral emotions, with a precision of 0.76 and a recall of 0.88. The weakest performance is observed for the unpleasant emotions, with a precision of 0.73 and a recall of 0.61. Overall, the model demonstrates decent performance, but there is room for improvement, particularly in the recognition of unpleasant emotions.","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\ncm = pd.DataFrame(cm)\n\nplt.figure(figsize = (12, 10))\nsns.heatmap(cm, xticklabels=label_names, yticklabels=label_names, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\nplt.title('Confusion Matrix', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:33:24.944287Z","iopub.execute_input":"2023-03-28T19:33:24.945015Z","iopub.status.idle":"2023-03-28T19:33:25.230784Z","shell.execute_reply.started":"2023-03-28T19:33:24.944967Z","shell.execute_reply":"2023-03-28T19:33:25.229518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TinyML Model Conversion & Evaluation\nTo deploy the speech emotion recognition model on microcontrollers, there are a few steps we have to do to fit in different hardware environments, including post-quantization and model format conversion.","metadata":{}},{"cell_type":"markdown","source":"First, we plot the histogram to visualize the model's weights. This step is essential because the conversion can cause accuracy loss due to the reduced precision of the weights. By examining the weight's distribution and range, we can get a sense of the effectiveness of the post-quantization process.","metadata":{}},{"cell_type":"code","source":"weights = model.get_weights()\n\n# Plot a histogram of the weights\nplt.hist(weights[0].flatten(), bins=50)\nplt.xlabel('Weight value')\nplt.ylabel('Frequency')\nplt.title('Histogram of model weights')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:33:25.234748Z","iopub.execute_input":"2023-03-28T19:33:25.235105Z","iopub.status.idle":"2023-03-28T19:33:25.45358Z","shell.execute_reply.started":"2023-03-28T19:33:25.235063Z","shell.execute_reply":"2023-03-28T19:33:25.452373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Second, we converts the trained model to a TensorFlow Lite format using the TensorFlow Lite framework, which can optimize the model for deployment on mobile devices. ","metadata":{}},{"cell_type":"code","source":"converter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\nwith tf.io.gfile.GFile(\"Models/SER.tflite\", 'wb') as f:\n   f.write(tflite_model)\n\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nquant_tflite_model = converter.convert()\nwith tf.io.gfile.GFile(\"Models/SER_quant.tflite\", 'wb') as f:\n   f.write(quant_tflite_model)\n\nprint(\"Save the Tensorflow 'Lite' model!\")","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:33:25.454975Z","iopub.execute_input":"2023-03-28T19:33:25.455282Z","iopub.status.idle":"2023-03-28T19:33:52.666794Z","shell.execute_reply.started":"2023-03-28T19:33:25.455252Z","shell.execute_reply":"2023-03-28T19:33:52.665797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Model Sizes:\")\n!ls -lh Models | awk '{print $5 \"\\t\" $9}'","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:33:52.667933Z","iopub.execute_input":"2023-03-28T19:33:52.668208Z","iopub.status.idle":"2023-03-28T19:33:53.79045Z","shell.execute_reply.started":"2023-03-28T19:33:52.668181Z","shell.execute_reply":"2023-03-28T19:33:53.789027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The goal of TinyML techniques is to reduce the model size and the inference time while maintaining similar accuracy. Therefore, we compare the inference time and accuracy of the original TensorFlow model with the TensorFlow Lite model.","metadata":{}},{"cell_type":"code","source":"def evaluate_tflite(interpreter, test_data, test_label):\n    # Get the input and output tensors.\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    \n    num_correct = 0\n    num_total = 0\n\n    # Iterate over the testing data.\n    for i in range(test_data.shape[0]):\n        # Get the input data for this example.\n        input_data = np.array([test_data[i]], dtype=np.float32)\n\n        # Set the input tensor.\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n\n        # Run inference.\n        interpreter.invoke()\n\n        # Get the output tensor.\n        output_data = interpreter.get_tensor(output_details[0]['index'])\n\n        # Compute the predicted label.\n        predicted_label = np.argmax(output_data)\n\n        # Update the results.\n        if predicted_label == test_label[i]:\n            num_correct += 1\n        num_total += 1\n\n    # Reset all variables so it will not pollute other inferences.\n    interpreter.reset_all_variables()\n    \n    # Compute the accuracy.\n    accuracy = num_correct / num_total\n    \n    return accuracy\n\n    \n# Load tflite model.\ninterpreter = tf.lite.Interpreter(model_path=\"Models/SER_quant.tflite\")\ninterpreter.allocate_tensors()\n\ntflite_test_acc = evaluate_tflite(interpreter, X_test, y_test)\nprint(f\"TF Lite Model Accuracy: {tflite_test_acc * 100:.2f}%\")\nprint(f\"Accuracy Difference from Original Model: {test_acc}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:33:53.792697Z","iopub.execute_input":"2023-03-28T19:33:53.793162Z","iopub.status.idle":"2023-03-28T19:33:55.764705Z","shell.execute_reply.started":"2023-03-28T19:33:53.793123Z","shell.execute_reply":"2023-03-28T19:33:55.763631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\ninput_data = np.random.randn(1, 47, 13).astype(np.float32)\n\nstart_time = time.time()\nfor i in range(100):\n    h5_predictions = model.predict(input_data)\nh5_inference_time = time.time() - start_time\n\nstart_time = time.time()\nfor i in range(100):\n    interpreter.set_tensor(interpreter.get_input_details()[0]['index'], input_data)\n    interpreter.invoke()\n    tflite_predictions = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])\n    interpreter.reset_all_variables()\ntflite_inference_time = time.time() - start_time\n\nprint(\"Inference Time Comparison:\")\nprint(f\"Original Model: {h5_inference_time}s\")\nprint(f\"TF Lite Model: {tflite_inference_time}s\")","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:33:55.766282Z","iopub.execute_input":"2023-03-28T19:33:55.766749Z","iopub.status.idle":"2023-03-28T19:34:01.063132Z","shell.execute_reply.started":"2023-03-28T19:33:55.766703Z","shell.execute_reply":"2023-03-28T19:34:01.062147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Previously, we converted the original model into a TensorFlow Lite format optimized for deployment on edge devices with limited computational resources. However, more is needed to deploy the model on a microcontroller. Therefore, we have to convert the TFLite file into a TFLite \"Micro\" file which can be uploaded to microcontrollers like Arduino Nano 33 BLE, enabling real-time SER applications with similar accuracy to the original model.","metadata":{}},{"cell_type":"code","source":"MODEL_TFLITE = 'Models/SER_quant.tflite'\nMODEL_TFLITE_MICRO = 'Models/SER_micro.cc'\n!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}\nREPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')\n!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}\nprint(\"Save the Tensorflow Lite 'Micro' model!\")","metadata":{"execution":{"iopub.status.busy":"2023-03-28T19:34:01.064677Z","iopub.execute_input":"2023-03-28T19:34:01.065111Z","iopub.status.idle":"2023-03-28T19:34:03.33547Z","shell.execute_reply.started":"2023-03-28T19:34:01.065064Z","shell.execute_reply":"2023-03-28T19:34:03.334201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**That's all about the project. Thank you for joining me on this journey!<br>\nFeel free to share your feedback—I would appreciate the chance to learn and grow together. Cheers!**","metadata":{}}]}